{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Georeferencing\n",
    "\n",
    "This tutorial takes on the challenge of georeferencing the imagery. To georeference, you need:\n",
    "\n",
    "a. Synchronized navigation data covering the time of the transect (or sub-transect chunk) in the form of 3 positions and 3 orientations.\n",
    "\n",
    "b. A camera model (we use *.XML files) describing boresight angles, lever arms, focal length, principal point, and distortion coefficients.\n",
    "\n",
    "c. A digital elevation/surface model, either in as orthographic \"*.tif\" file, or as a triangular mesh, \"*.ply\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We start by some imports\"\"\"\n",
    "# Standard python library\n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third party libraries\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "\n",
    "module_path = 'C:/Users/haavasl/VSCodeProjects/hyperspectral_toolchain/src/'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Local resources\n",
    "from lib import parsing_utils\n",
    "from scripts import visualize\n",
    "from scripts.modulate_config import prepend_data_dir_to_relative_paths\n",
    "from lib.parsing_utils import reformat_h5_embedded_data_h5\n",
    "from scripts.gis_tools import dem_2_mesh\n",
    "from scripts.geometry import CameraGeometry, CalibHSI\n",
    "from lib.parsing_utils import Hyperspectral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definitions of folder paths\n",
    "DISK_NAME = 'D:/'\n",
    "DATA_DIR = DISK_NAME + 'HyperspectralDataAll/HI/2022-08-31-060000-Remoy-Specim/'\n",
    "\n",
    "INPUT_DIR = DATA_DIR + 'Input/'\n",
    "H5_DIR = INPUT_DIR + 'H5/'\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR + 'Output/'\n",
    "config_file = DATA_DIR + 'configuration.ini'\n",
    "\n",
    "# Set the data directory for the mission (locally where the data is stored)\n",
    "prepend_data_dir_to_relative_paths(config_path=config_file, DATA_DIR = DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## a. Reading the poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n",
      "True\n",
      "Hello3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use updated config file to reformat the position and orientation data.\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\"\"\"What happens under the hood is that 3xN array of ECEF positions and euler angle orientations are converted to a specific geocentric (ECEF) format: \n",
    "They are also interpolated to the times of the hyperspectral frames.\"\"\"\n",
    "config = reformat_h5_embedded_data_h5(config=config,\n",
    "                                              config_file=config_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to ECEF quaternions is processed/nav/position_ref_ecef \n",
      "Path to ECEF positions is processed/nav/quaternion_ref_ecef\n"
     ]
    }
   ],
   "source": [
    "# After processing, the 3D positions for each transect chunk can be found under the hierarchical path\n",
    "pos_h5_path = config['HDF.processed_nav']['position_ecef']\n",
    "quat_h5_path = config['HDF.processed_nav']['quaternion_ecef']\n",
    "\n",
    "print('Path to ECEF quaternions is {0} \\nPath to ECEF positions is {1}'.format(pos_h5_path,quat_h5_path))\n",
    "\n",
    "# NB, note that to avoid rounding errors the positions in h5 file are subtracted a globally known offset:\n",
    "# If is is desired to inspect the data, h5 files \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Reading in the digital elevation model\n",
    "To use the georeferencing framework for ray casting on arbitrary 3D surfaces, we employ 3D triangular meshes. Therefore, in the case that you only have digital elevation model as tif, you will need to convert the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input projected DEM file. IMPORTANT: heights must be given with respect to ellipsoid\n",
    "file_path_dem = config['Absolute Paths']['dempath']\n",
    "\n",
    "# The output geocentric 3D model, which uses same CRS as poses\n",
    "file_path_3d_model = config['Absolute Paths']['modelpath']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some DEM's are given with respect to mean sea level (i.e. geoid). In that case adding the geoid height is necessary as:\n",
    "´´´\n",
    "dem_total = dem_terrain_wrt_geoid + dem_geoid\n",
    "´´´\n",
    "\n",
    "* dem_geoid: Geoid dems can be found at https://www.agisoft.com/downloads/geoids/ both global models (e.g. EGM 2008) and local models (e.g. NN2000 for Norway)\n",
    "* dem_terrain_wrt_geoid: Terrain models are often supplied by regional authority, norwegian mapping authority at https://hoydedata.no/LaserInnsyn2/. Alternatively, terrain models are supplied by aerial drones from SeaBee infrastructure (either as DSM or DEM or DTM)\n",
    "* For ocean color it is rational to use the geoid and tide level.\n",
    "* At present, refractive georeferencing is not implemented as it is inefficient for the adjustment procedure. When it is added as functionality, the DEMs must be corrected for refraction, and water depth utilized.\n",
    "* At present, Uncorrected photo-DEMs can be used as-is, while true bathymetry data e.g. LiDAR should be transformed \n",
    "$$\n",
    "h_{e,b}^{*} = h_{e,wl} + (h_{e,b} - h_{e,wl})\\frac{1}{n_{w}}\n",
    "$$\n",
    "* Where $h_{e,b}^{*}$ is the ellipsoid height in the transformed bathymetry DEM to be used onward, $h_{e,b}$ is the ellipsoid heights in the true bathymetry, $h_{e,wl}$ is the ellipsoid height of the water line (available from NMA) and $n_{w}$ is the refractive index of water, often 1.34 is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEM projected EPSG Code: 25832\n",
      "Conversion completed.\n",
      "Mesh geocentric EPSG Code: 4936\n"
     ]
    }
   ],
   "source": [
    "# No need for this step if a mesh model file already exists\n",
    "dem_2_mesh(path_dem=file_path_dem, model_path=file_path_3d_model, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more intuitive understanding, visualization of navigation data and positions/orientations is useful. It is also good for a sanity check to see that the data is interpreted correctly. For a regular airborne vehicle, the blue arrows should point towards the surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# For aerial imaging, looking at data in a east, north, up (ENU) frame makes most sense.\n",
    "# Pressing the \"Top(-Z)\" option is most illustrative as it gives an orthographic view with north upward, and east to the right.\n",
    "# The red arrow: forward/x-axis on Specim vehicle, aligned with vehicle's direction of motion and opposite of y-axis of HSI.\n",
    "# The green arrow: starboard/right/y-axis on Specim vehicle. Aligns with the x-axis of HSI.\n",
    "# The blue arrow: downward/z-axis on Specim vehicle. Aligns with the x-axis of HSI. Should point perfectly perpendicular to terrain surface.\n",
    "# Other available options for frame are NED and ECEF.\n",
    "\n",
    "\n",
    "#visualize.show_mesh_camera(config, show_mesh = True, show_pose = True, ref_frame = 'ENU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Reading in camera model\n",
    "The camera model's first component is a rigid body transform from 3 lever arm parameters and 3 boresight rotation angles:\n",
    "\n",
    "$R_{h}^{e}(t) = R_{i}^{e}(t) R_{h}^{i}\\\\\n",
    "p_{h/e}^{e} = p_{i/e}^{e} + R_{i}^{e}(t)p_{h/i}^{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calibration': {'rx': '0.0', 'ry': '0.0', 'rz': '-1.5707963267948966', 'tx': '0', 'ty': '0', 'tz': '0', 'f': '754.9669306613682', 'cx': '255.0099175768686', 'k1': '-72.31616804110381', 'k2': '-389.5781973543412', 'k3': '4.075384334827561', 'width': '512'}}\n",
      "[[ 0.  1.  0.]\n",
      " [-1.  0. -0.]\n",
      " [-0.  0.  1.]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "CAM_MODEL_PATH_XML = config['Absolute Paths']['hsicalibfile']\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "with open(CAM_MODEL_PATH_XML, 'r', encoding='utf-8') as xml_file:\n",
    "    my_xml = xml_file.read()\n",
    "    xml_dict = xmltodict.parse(my_xml)\n",
    "\n",
    "# For ease, print the XML file\n",
    "print(xml_dict)\n",
    "\n",
    "# Rotations\n",
    "rx = float(xml_dict['calibration']['rx'])\n",
    "ry = float(xml_dict['calibration']['ry'])\n",
    "rz = float(xml_dict['calibration']['rz'])\n",
    "eul_ZYX = np.array([rz, ry, rx])\n",
    "R_h_i = Rotation.from_euler('ZYX', eul_ZYX, degrees=False)\n",
    "\n",
    "\n",
    "# Translations describing p_{i/h} i.e. vector from HSI to IMU.\n",
    "tx = float(xml_dict['calibration']['tx'])\n",
    "ty = float(xml_dict['calibration']['ty'])\n",
    "tz = float(xml_dict['calibration']['tz'])\n",
    "p_i_h = np.array([tx, ty, tz]).reshape((-1))\n",
    "p_h_i = -p_i_h\n",
    "\n",
    "# Example of composition. Assume a ECEF position [0, 0, 0] and rotation matrix at identity I_3\n",
    "p_i_e = np.array([0, 0, 0]).reshape((-1))\n",
    "R_i_e = Rotation.from_matrix(np.eye(3))\n",
    "\n",
    "# The equations above thus becomes\n",
    "R_h_e = R_i_e*R_h_i\n",
    "p_h_e = p_i_e + R_i_e.apply(p_h_i)\n",
    "\n",
    "# Set the precision for printing\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(R_h_e.as_matrix())\n",
    "print(p_h_e)\n",
    "\n",
    "hsi_x_basis = R_h_i.as_matrix()[:,0] # Defaults to [0 -1 0]\n",
    "hsi_y_basis = R_h_i.as_matrix()[:,1]\n",
    "hsi_z_basis = R_h_i.as_matrix()[:,2]\n",
    "\n",
    "# TODO: Make process simpler to avoid confusion between typical computer vision frames (as used for HSI) and the typical navigation frames (i.e. vehicle body)\n",
    "\n",
    "# Note that writing, or modifying parameters can be done as:\n",
    "#xml_dict['calibration']['tx'] = value\n",
    "#with open(file_name_cal_xml, 'w') as fd:\n",
    "#    fd.write(xmltodict.unparse(xml_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Tide compensation\n",
    "\n",
    "To compensate for tide effects, it is necessary to either use an on-site measurement or data from national authorities. Below we illustrate with data from the norwegian mapping authorities (downloaded from online with vertival reference NN2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tide = os.path.join('..', '02_Georeferencing', 'data', 'tidevann_nn2000_NMA.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d. Start the actual georeferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-31-060000-Remoy-Specim_transectnr_0_chunknr_0.h5\n"
     ]
    }
   ],
   "source": [
    "# Paths to 3D mesh ply file \n",
    "path_mesh = config['Absolute Paths']['modelPath']\n",
    "\n",
    "# Directory of H5 files\n",
    "dir_r = config['Absolute Paths']['h5Dir']\n",
    "\n",
    "# The path to the XML file\n",
    "hsi_cal_xml = config['Absolute Paths']['HSICalibFile']\n",
    "\n",
    "# Maximal allowed ray length\n",
    "max_ray_length = float(config['General']['maxRayLength'])\n",
    "\n",
    "mesh = pv.read(path_mesh)\n",
    "\n",
    "h5_files = sorted(os.listdir(dir_r))\n",
    "\n",
    "# For illustration, let us use 1st file:\n",
    "h5_file = h5_files[0]\n",
    "\n",
    "print(h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ray Tracing\n",
      "Ray traced 1024000 rays on a mesh with 100674 cells in 0.6638753414154053 seconds\n",
      "Finished ray tracing\n"
     ]
    }
   ],
   "source": [
    "from scripts import georeference_mod as grf\n",
    "# Read h5 file\n",
    "\n",
    "hyp = Hyperspectral(dir_r + h5_file, config)\n",
    "\n",
    "# We can here access the interpolated position, orientation data, timestamps and the position offset (used to avoid rounding errors)\n",
    "pos_ref_ecef = hyp.pos_ref\n",
    "quat_ref_ecef = hyp.quat_ref\n",
    "time_pose = hyp.pose_time\n",
    "pos0 = hyp.pos0\n",
    "\n",
    "# Meaning that pos_ref_ecef_tot = pos_ref_ecef + pos0\n",
    "\n",
    "# Using the cal file, we can define lever arm, boresight and local ray geometry (in dictionary)\n",
    "intrinsic_geometry_dict = grf.cal_file_to_rays(filename_cal=hsi_cal_xml, config=config)\n",
    "\n",
    "# Using both intrinsic and extrinsic geometry, we can define all ray directions\n",
    "hsi_geometry = grf.define_hsi_ray_geometry(pos_ref_ecef, quat_ref_ecef, time_pose, pos0, intrinsic_geometry_dict)\n",
    "\n",
    "# hsi_geometry is an object holding the necessary geometry for performing the intersection:\n",
    "hsi_geometry.intersectWithMesh(mesh = mesh, max_ray_length=max_ray_length)\n",
    "\n",
    "\n",
    "\n",
    "# Writing an RGB point cloud version of the datacube (only for visualization, not very useful)\n",
    "hsi_geometry.writeRGBPointCloud(config = config, hyp = hyp, transect_string = h5_file.split('.')[0])\n",
    "\n",
    "# Computes the view angles in the local NED. Computationally intensive as local NED is defined for each intersection\n",
    "hsi_geometry.compute_view_directions_local_tangent_plane()\n",
    "\n",
    "# Computes the sun angles in the local NED. Computationally intensive as local NED is defined for each intersection\n",
    "hsi_geometry.compute_sun_angles_local_tangent_plane()\n",
    "\n",
    "# () Compute the tide level for each measurement, based on a tide file downloaded from the norwegian mapping authority\n",
    "hsi_geometry.compute_tide_level(path_tide, tide_format = 'NMA')\n",
    "\n",
    "#visualize.show_projected_hsi_points(HSICameraGeometry=hsi_geometry, config=config, transect_string = h5_file.split('.')[0])\n",
    "\n",
    "# TODO We should also compute the water depth from e.g. LIDAR, although this could perhaps be computed easier for rectified data\n",
    "# Simplest be a function \n",
    "\n",
    "\n",
    "# Writing intersection geometry to the h5 file\n",
    "grf.write_intersection_geometry_2_h5_file(hsi_geometry=hsi_geometry, config = config, h5_filename=dir_r + h5_file)\n",
    "\n",
    "visualize.show_projected_hsi_points(HSICameraGeometry=hsi_geometry, config=config, transect_string = h5_file.split('.')[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Example calculation of solar angle, verified against NOAA: https://gml.noaa.gov/grad/solcalc/\\nfrom datetime import datetime\\n\\nlon = hsi_geometry.lons.mean()\\nprint(lon)\\n\\nlat = hsi_geometry.lats.mean()\\nprint(lat)\\n\\ntime_unix = hsi_geometry.time.mean()\\nprint(time_unix)\\n\\nprint(datetime.utcfromtimestamp(time_unix))\\n\\nphi_s, theta_s = CameraGeometry.calculate_sun_directions(lon, lat, 0, time_unix)\\n\\n\\nprint(f'{phi_s} is the azimuth and {90-theta_s} is the elevation angle')\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Example calculation of solar angle, verified against NOAA: https://gml.noaa.gov/grad/solcalc/\n",
    "from datetime import datetime\n",
    "\n",
    "lon = hsi_geometry.lons.mean()\n",
    "print(lon)\n",
    "\n",
    "lat = hsi_geometry.lats.mean()\n",
    "print(lat)\n",
    "\n",
    "time_unix = hsi_geometry.time.mean()\n",
    "print(time_unix)\n",
    "\n",
    "print(datetime.utcfromtimestamp(time_unix))\n",
    "\n",
    "phi_s, theta_s = CameraGeometry.calculate_sun_directions(lon, lat, 0, time_unix)\n",
    "\n",
    "\n",
    "print(f'{phi_s} is the azimuth and {90-theta_s} is the elevation angle')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Do this for all files\\nfor h5_file in h5_files:\\n    hyp = Hyperspectral(dir_r + h5_file, config)\\n\\n    # We can here access the interpolated position, orientation data, timestamps and the position offset (used to avoid rounding errors)\\n    pos_ref_ecef = hyp.pos_ref\\n    quat_ref_ecef = hyp.quat_ref\\n    time_pose = hyp.pose_time\\n    pos0 = hyp.pos0\\n\\n    # Meaning that pos_ref_ecef_tot = pos_ref_ecef + pos0\\n\\n    # Using the cal file, we can define lever arm, boresight and local ray geometry (in dictionary)\\n    intrinsic_geometry_dict = grf.cal_file_to_rays(filename_cal=hsi_cal_xml, config=config)\\n\\n    # Using both intrinsic and extrinsic geometry, we can define all ray directions\\n    hsi_geometry = grf.define_hsi_ray_geometry(pos_ref_ecef, quat_ref_ecef, time_pose, pos0, intrinsic_geometry_dict)\\n\\n    # hsi_geometry is an object holding the necessary geometry for performing the intersection:\\n    hsi_geometry.intersectWithMesh(mesh = mesh, max_ray_length=max_ray_length)\\n\\n    # Calculate the earth-sun vector per scanline and the view directions in NED\\n    hsi_geometry.compute_view_directions_local_tangent_plane()\\n    # Writing intersection geometry to the h5 file\\n    grf.write_intersection_geometry_2_h5_file(hsi_geometry=hsi_geometry, config = config, h5_filename=dir_r + h5_file)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do this for all files\n",
    "for h5_file in h5_files:\n",
    "    hyp = Hyperspectral(dir_r + h5_file, config)\n",
    "\n",
    "    # We can here access the interpolated position, orientation data, timestamps and the position offset (used to avoid rounding errors)\n",
    "    pos_ref_ecef = hyp.pos_ref\n",
    "    quat_ref_ecef = hyp.quat_ref\n",
    "    time_pose = hyp.pose_time\n",
    "    pos0 = hyp.pos0\n",
    "\n",
    "    # Meaning that pos_ref_ecef_tot = pos_ref_ecef + pos0\n",
    "\n",
    "    # Using the cal file, we can define lever arm, boresight and local ray geometry (in dictionary)\n",
    "    intrinsic_geometry_dict = grf.cal_file_to_rays(filename_cal=hsi_cal_xml, config=config)\n",
    "\n",
    "    # Using both intrinsic and extrinsic geometry, we can define all ray directions\n",
    "    hsi_geometry = grf.define_hsi_ray_geometry(pos_ref_ecef, quat_ref_ecef, time_pose, pos0, intrinsic_geometry_dict)\n",
    "\n",
    "    # hsi_geometry is an object holding the necessary geometry for performing the intersection:\n",
    "    hsi_geometry.intersectWithMesh(mesh = mesh, max_ray_length=max_ray_length)\n",
    "\n",
    "    # Calculate the earth-sun vector per scanline and the view directions in NED\n",
    "    hsi_geometry.compute_view_directions_local_tangent_plane()\n",
    "    # Writing intersection geometry to the h5 file\n",
    "    grf.write_intersection_geometry_2_h5_file(hsi_geometry=hsi_geometry, config = config, h5_filename=dir_r + h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the point-cloud georeferences further, they "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
